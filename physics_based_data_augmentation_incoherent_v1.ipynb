{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aa58a1c",
   "metadata": {},
   "source": [
    "## Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import ignite\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef379545",
   "metadata": {},
   "source": [
    "## Weighted MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd457a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_weighted_mse(y_pred, y, loss_weights):\n",
    "    y_diff_squared = (y - y_pred)**2\n",
    "    y_diff_squared_weighted = torch.einsum('ijkl,j->ijkl', y_diff_squared, exp_mse_weights)\n",
    "    weighted_mse = torch.mean(y_diff_squared_weighted)\n",
    "    return weighted_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce745e0",
   "metadata": {},
   "source": [
    "## Generate weights for exponential MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_exp_weights(T, alpha):\n",
    "    decay_length = 20.0\n",
    "    augmentation_container = torch.linspace(0, 1, steps=T)\n",
    "    augmentation_parameter = torch.linspace(0, alpha, steps=T)\n",
    "    exp_mse_weights_unflipped = torch.exp(-decay_length * augmentation_container)\n",
    "    exp_mse_weights = torch.flip(exp_mse_weights_unflipped, dims=[0])\n",
    "    return exp_mse_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810c7721",
   "metadata": {},
   "source": [
    "## PSF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3fbb9",
   "metadata": {},
   "source": [
    "### Gaussian PSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e1f9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_psf_kernel(sigma=10.0, psf_size=30):\n",
    "#def generate_psf_kernel(sigma=1.0, psf_size=15):\n",
    "\n",
    "    psf_kernel = torch.zeros(psf_size, psf_size)\n",
    "    psf_center = psf_size // 2\n",
    "    for x in range(psf_size):\n",
    "        for y in range(psf_size):\n",
    "            psf_kernel[x, y] = torch.exp(torch.tensor(-((x - psf_center) ** 2 + (y - psf_center) ** 2) / (2 * sigma ** 2)))\n",
    "    psf_kernel /= psf_kernel.sum()\n",
    "\n",
    "    return psf_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b37b4b",
   "metadata": {},
   "source": [
    "### Literature PSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73648419",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_psf_lit = False\n",
    "if use_psf_lit:\n",
    "    psf_lit_image_path = \"psf.png\"\n",
    "    psf_lit_image_pil = Image.open(psf_lit_image_path)\n",
    "    psf_lit_kernel = transform(psf_lit_image_pil).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f01d0d",
   "metadata": {},
   "source": [
    "### Choose PSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c0670",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_psf_lit:\n",
    "    psf_kernel = psf_lit_kernel\n",
    "else:\n",
    "    psf_kernel = generate_psf_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f36172",
   "metadata": {},
   "source": [
    "## Define CoordConv2D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e8ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordConv2DLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        _, _, height, width = input_tensor.size()\n",
    "\n",
    "        # Create x and y coordinate grids\n",
    "        xx_channel = torch.arange(width).view(1, 1, 1, width).expand(1, 1, height, width).float() / (width - 1)\n",
    "        yy_channel = torch.arange(height).view(1, 1, height, 1).expand(1, 1, height, width).float() / (height - 1)\n",
    "\n",
    "        # Concatenate the coordinate channels to the input tensor\n",
    "        output_tensor = torch.cat([xx_channel, yy_channel], dim=1)\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec369ae",
   "metadata": {},
   "source": [
    "## Define FourierConv2D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdcfd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierConv2DLayer(nn.Module):\n",
    "    def __init__(self, L):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, num_input_channels, height, width = x.size()\n",
    "\n",
    "        # Generate frequencies\n",
    "        base_frequency = 2\n",
    "        exponent_value = torch.arange(L)\n",
    "        frequencies = torch.pow(torch.tensor(base_frequency), exponent_value).float()\n",
    "\n",
    "        # Apply Fourier basis functions\n",
    "        fourier_features = [torch.sin(frequencies[j] * torch.pi * x) for j in range(L)]\n",
    "        fourier_features += [torch.cos(frequencies[j] * torch.pi * x) for j in range(L)]\n",
    "\n",
    "        # Concatenate the Fourier features along the channel dimension\n",
    "        fourier_features = torch.cat(fourier_features, dim=1)\n",
    "\n",
    "        return fourier_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e02e1ff",
   "metadata": {},
   "source": [
    "## Define InverseConv2D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d6644",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseConv2DLayer(nn.Module):\n",
    "    def __init__(self, L, num_standard_layers, max_reflectance, subsample_factor=2, use_fourier=True):\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.num_fourier_channels = 2\n",
    "        self.num_standard_layers = num_standard_layers\n",
    "        self.num_filters = 128\n",
    "        self.use_fourier = use_fourier\n",
    "        \n",
    "        self.upsample_layer = nn.Upsample(scale_factor=subsample_factor, mode='nearest')\n",
    "        self.coordinate_layer = CoordConv2DLayer()\n",
    "        self.fourier_layer = FourierConv2DLayer(L)\n",
    "            \n",
    "        if self.use_fourier:\n",
    "            self.fourier_layer = FourierConv2DLayer(L)\n",
    "            self.num_fourier_channels = 4*L\n",
    "        \n",
    "        self.standard_hidden_layers = nn.ModuleList(\n",
    "        [nn.Conv2d(self.num_fourier_channels, self.num_filters, kernel_size=3, padding='same')\n",
    "         if c == 0 else\n",
    "         nn.Conv2d(self.num_filters, self.num_filters, kernel_size=3, padding='same')\n",
    "         for c in range(num_standard_layers)]\n",
    "        )\n",
    "        \n",
    "        self.standard_output_layer = nn.Conv2d(self.num_filters, 1, kernel_size=3, padding='same')\n",
    "        \n",
    "        self.downsample_layer = nn.MaxPool2d(kernel_size=subsample_factor, stride=subsample_factor)\n",
    "        \n",
    "        # Initialize weights with He uniform variance scaling initializer\n",
    "        for layer in self.standard_hidden_layers:\n",
    "            nn.init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "            nn.init.zeros_(layer.bias)  # Initialize biases to zero\n",
    "        nn.init.kaiming_uniform_(self.standard_output_layer.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        nn.init.zeros_(self.standard_output_layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample_layer(x)\n",
    "        output_coordinate = self.coordinate_layer(x)\n",
    "        \n",
    "        if self.use_fourier:\n",
    "            output_fourier = self.fourier_layer(output_coordinate)\n",
    "        else:\n",
    "            output_fourier = output_coordinate\n",
    "        \n",
    "        x = output_fourier\n",
    "        for layer in self.standard_hidden_layers:\n",
    "            x = nn.functional.elu(layer(x))\n",
    "        \n",
    "        output = nn.functional.softplus(self.standard_output_layer(x))\n",
    "        \n",
    "        use_sigmoidal_output = True\n",
    "        if use_sigmoidal_output:\n",
    "            output = max_reflectance * torch.sigmoid(output)\n",
    "        \n",
    "        output_downsampled = self.downsample_layer(output)\n",
    "        output_inverse = output_downsampled\n",
    "\n",
    "        return output_coordinate, output_fourier, output, output_inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7e4540",
   "metadata": {},
   "source": [
    "## Define Augmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(original_image, augmentation_stride=1, blob_intensity=0.01, contrast_steps=2, mode=\"contrast\"):\n",
    "    \n",
    "    augmented_image = torch.zeros_like(original_image)\n",
    "    \n",
    "    if mode == \"translation\":\n",
    "        augmentation_stride = min(augmentation_stride, 19)\n",
    "        augmented_image[:, :, :, augmentation_stride:] = original_image[:, :, :, :-augmentation_stride]\n",
    "    elif mode == \"crop\":\n",
    "        _, num_input_channels, height, width = original_image.size()\n",
    "        win_size = 10\n",
    "        #win_size = 50\n",
    "        row_start = win_size\n",
    "        row_end = height - win_size\n",
    "        col_start = win_size\n",
    "        col_end = width - win_size\n",
    "        row_range = np.arange(row_start, row_end)\n",
    "        col_range = np.arange(col_start, col_end)\n",
    "        row, col = np.meshgrid(row_range, col_range, indexing='ij')\n",
    "        idx = np.stack([row, col], axis=-1)\n",
    "        idx = idx.reshape((-1, idx.shape[-1]))\n",
    "        center = [idx_iter for iter, idx_iter in enumerate(idx)]\n",
    "        #augmentation_idx = 10*(augmentation_stride - 1)\n",
    "        augmentation_idx = augmentation_stride - 1\n",
    "        #augmented_image = torch.zeros(1, num_input_channels, 2*win_size, 2*win_size)\n",
    "        augmented_image[:, :, center[augmentation_idx][0]-win_size:center[augmentation_idx][0]+win_size, center[augmentation_idx][1]-win_size:center[augmentation_idx][1]+win_size] = original_image[:, :, center[augmentation_idx][0]-win_size:center[augmentation_idx][0]+win_size, center[augmentation_idx][1]-win_size:center[augmentation_idx][1]+win_size]\n",
    "    elif mode == \"elastic\":\n",
    "        elastic_parameter = 50.0 + augmentation_stride*10.0\n",
    "        elastic_transformer = transforms.ElasticTransform(alpha=elastic_parameter)\n",
    "        augmented_image = elastic_transformer(original_image)\n",
    "    elif mode == \"blob\":\n",
    "        blob_start = augmentation_stride\n",
    "        blob_size = 5\n",
    "        blob_end = blob_start + blob_size\n",
    "        blob_tensor = torch.zeros(original_image.size())\n",
    "        blob_tensor[:, :, blob_start:blob_end, blob_start:blob_end] = blob_intensity\n",
    "        augmented_image = original_image + blob_tensor\n",
    "    elif mode == \"contrast\":\n",
    "        reflectance_max = float(torch.max(original_image).detach().numpy())\n",
    "        reflectance_axis = torch.linspace(0, reflectance_max, steps=contrast_steps)\n",
    "        reflectance_augmented = reflectance_axis[augmentation_stride-1]\n",
    "        augmented_image = torch.abs(original_image - reflectance_augmented)\n",
    "    elif mode == \"contrast_and_blob\":\n",
    "        blob_start = augmentation_stride\n",
    "        blob_size = 5\n",
    "        blob_end = blob_start + blob_size\n",
    "        blob_tensor = torch.zeros(original_image.size())\n",
    "        blob_tensor[:, :, blob_start:blob_end, blob_start:blob_end] = blob_intensity\n",
    "        reflectance_max = float(torch.max(original_image).detach().numpy())\n",
    "        reflectance_axis = torch.linspace(0, reflectance_max, steps=contrast_steps)\n",
    "        reflectance_augmented = reflectance_axis[augmentation_stride-1]\n",
    "        augmented_image = torch.abs(original_image - reflectance_augmented) + blob_tensor\n",
    "    elif mode == \"background\":\n",
    "        background_max = float(torch.max(original_image).detach().numpy())\n",
    "        background_axis = torch.linspace(0, background_max, steps=contrast_steps)\n",
    "        background_augmented = background_axis[augmentation_stride-1]\n",
    "        augmented_image = original_image + background_augmented\n",
    "    \n",
    "    return augmented_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac712ef",
   "metadata": {},
   "source": [
    "## Define AugmentationConv2D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9476eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationConv2DLayer(nn.Module):\n",
    "    def __init__(self, T):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "\n",
    "    def forward(self, x, alpha):\n",
    "        _, num_input_channels, height, width = x.size()\n",
    "\n",
    "        # Apply augmentation\n",
    "        augmented_features = [\n",
    "                augment_image(x, augmentation_stride=j+1, blob_intensity=alpha, contrast_steps=self.T) for j in range(self.T)\n",
    "                ]\n",
    "\n",
    "        # Concatenate the augmented features along the channel dimension\n",
    "        augmented_features = torch.cat(augmented_features, dim=1)\n",
    "\n",
    "        return augmented_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ebf4f7",
   "metadata": {},
   "source": [
    "## Define Microscope CNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MicroscopeCNNLayer(nn.Module):\n",
    "    def __init__(self, psf_kernel):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Conv2d(1, 1, kernel_size=psf_kernel.size(0), padding='same', bias=False)\n",
    "        self.conv_layer.weight = nn.Parameter(psf_kernel.unsqueeze(0).unsqueeze(0), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_conv = self.conv_layer(x)\n",
    "        output_intensity = output_conv\n",
    "        output_final = output_intensity / torch.max(output_intensity)\n",
    "        return output_conv, output_intensity, output_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c350b0a",
   "metadata": {},
   "source": [
    "## Define PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631440bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, L, T, alpha, num_standard_layers, max_reflectance, subsample_factor, psf_kernel, use_fourier=True):\n",
    "        super().__init__()\n",
    "        self.inverse_layer = InverseConv2DLayer(L, num_standard_layers, max_reflectance, subsample_factor, use_fourier=use_fourier)\n",
    "        self.augmentation_layer = AugmentationConv2DLayer(T)\n",
    "        self.forward_layer = MicroscopeCNNLayer(psf_kernel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_coordinate, output_fourier, output, output_inverse = self.inverse_layer(x)\n",
    "        output_augmentation = self.augmentation_layer(output_inverse, alpha)\n",
    "        \n",
    "        batch_size, num_augmentations, height, width = output_augmentation.size()\n",
    "        \n",
    "        output_conv = torch.zeros_like(output_augmentation)\n",
    "        output_intensity = torch.zeros_like(output_augmentation)\n",
    "        output_final = torch.zeros_like(output_augmentation)\n",
    "        \n",
    "        for t in range(self.augmentation_layer.T):\n",
    "            output_conv_t, output_intensity_t, output_final_t = self.forward_layer(output_augmentation[:,t,:,:].unsqueeze(0))\n",
    "            output_conv[:,t,:,:] = output_conv_t\n",
    "            output_intensity[:,t,:,:] = output_intensity_t\n",
    "            output_final[:,t,:,:] = output_final_t\n",
    "        \n",
    "        return output, output_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe2cb2",
   "metadata": {},
   "source": [
    "## Sensor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d695854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensor_func(image, noise_level=0.1, subsample_factor=2):\n",
    "    # Generate random noise with the same shape as the input image\n",
    "    noise = noise_level * torch.randn_like(image)\n",
    "\n",
    "    # Add the scaled noise to the original image\n",
    "    noisy_image = image + noise\n",
    "    \n",
    "    # Apply subsampling using a pooling operation (e.g., MaxPool2d)\n",
    "    subsampled_image = nn.functional.max_pool2d(noisy_image, kernel_size=subsample_factor, stride=subsample_factor)\n",
    "    \n",
    "    # Clip the values to ensure they are within the valid range (0, 1)\n",
    "    sensor_image = torch.clamp(subsampled_image, 0, 1)\n",
    "\n",
    "    return sensor_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8aceab",
   "metadata": {},
   "source": [
    "## Extract sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8239f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "#sample_image_path = \"shepp_logan_phantom.png\"\n",
    "#sample_image_path = \"wavy_fibers_processed.png\"\n",
    "sample_image_path = \"wavy_fibers_processed_2.png\"\n",
    "sample_image_pil = Image.open(sample_image_path)\n",
    "sample_image_not_normalized = transform(sample_image_pil)\n",
    "sample_image = sample_image_not_normalized / torch.max(sample_image_not_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27973f65",
   "metadata": {},
   "source": [
    "## Visualize sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e96ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample_image.squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49b74e6",
   "metadata": {},
   "source": [
    "## Construct Ground Truth Reflectance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6fd8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_pixel_intensity = sample_image\n",
    "alpha = 1.0\n",
    "reflectance_ground_truth = alpha*normalized_pixel_intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce274863",
   "metadata": {},
   "source": [
    "## Resize Ground Truth Reflectance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b114bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels, height, width = reflectance_ground_truth.size()\n",
    "reflectance_ground_truth = reflectance_ground_truth.view(1, channels, height, width)\n",
    "reflectance_ground_truth.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aef22c6",
   "metadata": {},
   "source": [
    "## Generate \"Training\" Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5bd338",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_model_mismatch = False\n",
    "if apply_model_mismatch:\n",
    "    if use_psf_lit:\n",
    "        psf_kernel_real = psf_lit_kernel\n",
    "    else:\n",
    "        psf_kernel_real = generate_psf_kernel(sigma=2.0, psf_size=21)\n",
    "    microscope_model = MicroscopeCNNLayer(psf_kernel_real)\n",
    "else:\n",
    "    microscope_model = MicroscopeCNNLayer(psf_kernel)\n",
    "\n",
    "subsample_factor = 2\n",
    "apply_sensor = True\n",
    "\n",
    "_, _, unaugmented_output = microscope_model(reflectance_ground_truth)\n",
    "if apply_sensor:\n",
    "    unaugmented_output = sensor_func(unaugmented_output, subsample_factor = subsample_factor)\n",
    "else:\n",
    "    subsample_factor = 1\n",
    "\n",
    "#T = 2\n",
    "#T = 5\n",
    "T = 3\n",
    "\n",
    "augmentation_layer = AugmentationConv2DLayer(T)\n",
    "input_augmentation = augmentation_layer(reflectance_ground_truth, alpha)\n",
    "\n",
    "batch_size, num_augmentations, height, width = input_augmentation.size()\n",
    "training_image = torch.zeros(batch_size, num_augmentations, height//subsample_factor, width//subsample_factor)\n",
    "\n",
    "use_output_augmentation = True\n",
    "if use_output_augmentation:\n",
    "    augmentation_axis = torch.linspace(0, alpha, steps=T)\n",
    "    for t in range(T):\n",
    "        augmentation_parameter = augmentation_axis[t]\n",
    "        training_image_t = torch.abs(unaugmented_output - augmentation_parameter)\n",
    "        training_image[:,t,:,:] = training_image_t\n",
    "else: \n",
    "    for t in range(T):\n",
    "        _, _, training_image_t = microscope_model(input_augmentation[:,t,:,:].unsqueeze(0))\n",
    "        if apply_sensor:\n",
    "            training_image_t = sensor_func(training_image_t, subsample_factor = subsample_factor)\n",
    "        training_image[:,t,:,:] = training_image_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd3c2a7",
   "metadata": {},
   "source": [
    "## Check Training Image shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a936c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd005adb",
   "metadata": {},
   "source": [
    "## Visualize Training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, T, figsize=(15, 3))\n",
    "for t in range(T):\n",
    "    axs[t].imshow(training_image[:,t,:,:].squeeze().detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1895a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "unaugmented_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(training_image[:,0,:,:].squeeze().detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38771d0",
   "metadata": {},
   "source": [
    "## Define SSIM Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b87888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_loss(y_pred, y_true):\n",
    "    \n",
    "    ssim_metric = ignite.metrics.SSIM(data_range = 1.0)\n",
    "    ssim_metric.update((y_pred, y_true))\n",
    "    ssim_value = ssim_metric.compute()\n",
    "    ssim_loss_value = 1.0 - ssim_value\n",
    "    ssim_loss_tensor = torch.tensor(ssim_loss_value, requires_grad=True)\n",
    "    \n",
    "    return ssim_loss_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ba5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_loss(training_image, training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e8fff",
   "metadata": {},
   "source": [
    "## Define PSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e3a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(y_pred, y_true):\n",
    "    \n",
    "    psnr_metric = ignite.metrics.PSNR(data_range = 1.0)\n",
    "    psnr_metric.update((y_pred, y_true))\n",
    "    psnr_value = psnr_metric.compute()\n",
    "    \n",
    "    return psnr_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc35006",
   "metadata": {},
   "source": [
    "## Define configurations for Inverse Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b58ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 4\n",
    "num_standard_layers = 4\n",
    "max_reflectance = alpha\n",
    "use_fourier = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc7a03b",
   "metadata": {},
   "source": [
    "## Network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae46f6",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_model_dummy = PINN(L, T, alpha, num_standard_layers, max_reflectance, subsample_factor, psf_kernel, use_fourier)\n",
    "_, channels_dummy, height_dummy, width_dummy = training_image.size()\n",
    "summary(pinn_model_dummy, input_size=(channels_dummy, height_dummy, width_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b11a02",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04cf14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PINN model\n",
    "pinn_model = PINN(L, T, alpha, num_standard_layers, max_reflectance, subsample_factor, psf_kernel, use_fourier)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(pinn_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Set up the exponential learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=np.exp(np.log(5e-6 / 1e-4) / 10000))\n",
    "\n",
    "# Set the regularization strengths (lambdas)\n",
    "lambda_boundary = 0.25\n",
    "lambda_structural = 0.01\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 10000\n",
    "\n",
    "# Define list to store loss values\n",
    "loss_list = []\n",
    "\n",
    "# Define list to store intermediate outputs for reflectance\n",
    "intermediate_output_reflectance = []\n",
    "\n",
    "# Generate exp mse loss weights\n",
    "use_exp_mse_weights = False\n",
    "if use_exp_mse_weights:\n",
    "    exp_mse_weights = generate_exp_weights(T, alpha)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    iter_reflectance, iter_image = pinn_model(training_image)\n",
    "\n",
    "    # Calculate the mse loss\n",
    "    if use_exp_mse_weights:\n",
    "        loss_mse = loss_weighted_mse(iter_image, training_image, exp_mse_weights)\n",
    "    else:\n",
    "        loss_mse = criterion(iter_image, training_image)\n",
    "        \n",
    "    use_structural_regularization = False\n",
    "    if use_structural_regularization:\n",
    "        loss_ssim = ssim_loss(iter_image, training_image)\n",
    "        loss_mse = loss_mse + lambda_structural * loss_ssim   \n",
    "    \n",
    "    # Calculate the boundary regularization loss\n",
    "    loss_boundary = lambda_boundary * (\n",
    "          torch.square(iter_image[:, :, 0, :] - 0).mean()\n",
    "        + torch.square(iter_image[:, :, -1, :] - 0).mean()\n",
    "        + torch.square(iter_image[:, :, :, 0] - 0).mean()\n",
    "        + torch.square(iter_image[:, :, :, -1] - 0).mean()\n",
    "    )\n",
    "    \n",
    "    # Calculate the total loss\n",
    "    use_boundary_loss = False\n",
    "    if use_boundary_loss:\n",
    "        loss_total = loss_mse + loss_boundary\n",
    "    else:\n",
    "        loss_total = loss_mse\n",
    "    loss = loss_total\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print training statistics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "    loss_list.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b1abf",
   "metadata": {},
   "source": [
    "## Display Loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273cd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss History')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f13590",
   "metadata": {},
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f40c6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_reflectance, _ = pinn_model(training_image)\n",
    "_, _, predicted_image = microscope_model(predicted_reflectance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73203d2f",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a236181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_plot = reflectance_ground_truth.squeeze().numpy()\n",
    "predicted_reflectance_plot = predicted_reflectance.squeeze().detach().numpy()\n",
    "unaugmented_output_plot = unaugmented_output.squeeze().detach().numpy()\n",
    "\n",
    "fig, (ax_1, ax_2, ax_3) = plt.subplots(1, 3, figsize=(14, 6))\n",
    "fig.suptitle(\"Summary\")\n",
    "\n",
    "ax_1.imshow(ground_truth_plot, cmap='gray')\n",
    "#ax_1.imshow(ground_truth_plot, cmap='inferno')\n",
    "ax_1.set_ylabel('Y-coordinate')\n",
    "ax_1.set_title('Ground Truth')\n",
    "\n",
    "ax_2.imshow(unaugmented_output_plot, cmap='gray')\n",
    "#ax_2.imshow(unaugmented_output_plot, cmap='inferno')\n",
    "ax_2.set_title('Unaugmented Output')\n",
    "ax_2.set_xlabel('X-coordinate')\n",
    "\n",
    "ax_3.imshow(predicted_reflectance_plot, cmap='gray')\n",
    "#ax_3.imshow(predicted_reflectance_plot, cmap='inferno')\n",
    "ax_3.set_title('PINN Prediction')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c42364",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_1, ax_2) = plt.subplots(1, 2, figsize=(10, 6))\n",
    "fig.suptitle(\"Visualize Reflectance\")\n",
    "\n",
    "#ax_1.imshow(ground_truth_plot, cmap='gray')\n",
    "ax_1.imshow(ground_truth_plot, cmap='inferno')\n",
    "ax_1.set_xlabel('X-coordinate')\n",
    "ax_1.set_ylabel('Y-coordinate')\n",
    "ax_1.set_title('Ground Truth')\n",
    "\n",
    "#ax_2.imshow(predicted_reflectance_plot, cmap='gray')\n",
    "ax_2.imshow(predicted_reflectance_plot, cmap='inferno')\n",
    "ax_2.set_title('PINN Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4127fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_reflectance.shape, reflectance_ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccba838",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_loss(predicted_reflectance, reflectance_ground_truth[:, :, 1:, :])\n",
    "#ssim_loss(predicted_reflectance, reflectance_ground_truth[:, :, :, 1:])\n",
    "#ssim_loss(predicted_reflectance, reflectance_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e14595",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_subsampled = nn.functional.max_pool2d(reflectance_ground_truth, kernel_size=subsample_factor, stride=subsample_factor)\n",
    "ground_truth_subsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_loss(unaugmented_output, ground_truth_subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87242b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr(unaugmented_output, ground_truth_subsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e0b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr(predicted_reflectance, reflectance_ground_truth[:, :, :-1, :])\n",
    "#psnr(predicted_reflectance, reflectance_ground_truth[:, :, :, 1:])\n",
    "#psnr(predicted_reflectance, reflectance_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31295d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18476f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_reflectance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(predicted_image.squeeze().detach().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e7187",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = torch.meshgrid(torch.arange(predicted_reflectance_plot.shape[0]), torch.arange(predicted_reflectance_plot.shape[1]))\n",
    "\n",
    "fig= plt.figure(figsize=(10, 6))\n",
    "fig.suptitle(\"Surface plot of the Predicted Reflectance\")\n",
    "\n",
    "ax_1 = fig.add_subplot(111, projection='3d')\n",
    "surface = ax_1.plot_surface(x, y, predicted_reflectance_plot, cmap='viridis')\n",
    "plt.xlabel('X-coordinate')\n",
    "plt.ylabel('Y-coordinate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg, yg = torch.meshgrid(torch.arange(ground_truth_plot.shape[0]), torch.arange(ground_truth_plot.shape[1]))\n",
    "\n",
    "fig= plt.figure(figsize=(10, 6))\n",
    "fig.suptitle(\"Surface plot of the Ground Truth\")\n",
    "\n",
    "ax_1 = fig.add_subplot(111, projection='3d')\n",
    "surface = ax_1.plot_surface(xg, yg, ground_truth_plot, cmap='viridis')\n",
    "plt.xlabel('X-coordinate')\n",
    "plt.ylabel('Y-coordinate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cf8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_reflectance_plot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b96de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_plot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a932c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_plot = predicted_reflectance_plot - ground_truth_plot\n",
    "error_plot = predicted_reflectance_plot - ground_truth_plot[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5590081",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= plt.figure(figsize=(10, 6))\n",
    "fig.suptitle(\"Surface plot of the Error\")\n",
    "\n",
    "ax_1 = fig.add_subplot(111, projection='3d')\n",
    "surface = ax_1.plot_surface(x, y, error_plot, cmap='viridis')\n",
    "plt.xlabel('X-coordinate')\n",
    "plt.ylabel('Y-coordinate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(error_plot)\n",
    "plt.xlabel('error value')\n",
    "plt.ylabel('frequency')\n",
    "plt.title(\"Error Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dad9795",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.square(error_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890e427",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error_array = unaugmented_output - ground_truth_subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error_plot = training_error_array.squeeze().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7ba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(training_error_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327736d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.square(training_error_plot))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
